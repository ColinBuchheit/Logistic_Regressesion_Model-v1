{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('unseen.csv')\n",
    "\n",
    "# Mapping for 'Month' column\n",
    "df['Month'] = df['Month'].map({\n",
    "    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "    'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "})\n",
    "\n",
    "# Mapping for 'VisitorType' column\n",
    "df['VisitorType'] = df['VisitorType'].map({\n",
    "    'Returning_Visitor': 1, 'New_Visitor': 2, 'Other': 3\n",
    "})\n",
    "\n",
    "# Mapping for 'Weekend' and 'Revenue' columns\n",
    "df['Revenue'] = df['Revenue'].astype(str).str.strip().str.upper()\n",
    "df['Weekend'] = df['Weekend'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Apply the correct mapping\n",
    "df['Revenue'] = df['Revenue'].map({'TRUE': 1, 'FALSE': 0})\n",
    "df['Weekend'] = df['Weekend'].map({'TRUE': 1, 'FALSE': 0})\n",
    "\n",
    "# Handle missing values by filling with 0 (if any)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Check class distribution before resampling\n",
    "print(f\"Class distribution before resampling: \\n{df['Revenue'].value_counts()}\")\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df.Revenue == 0]\n",
    "df_minority = df[df.Revenue == 1]\n",
    "\n",
    "# Upsample the minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,  # Sample with replacement\n",
    "                                 n_samples=len(df_majority),  # Match majority class size\n",
    "                                 random_state=42)\n",
    "\n",
    "# Combine the majority class with the upsampled minority class\n",
    "df_balanced = pd.concat([df_majority, df_minority_upsampled]).sample(frac=1, random_state=42)\n",
    "\n",
    "# Confirm the new class balance\n",
    "print(f\"Balanced class distribution: \\n{df_balanced['Revenue'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceed with train-test split\n",
    "X = df_balanced.iloc[:, :-1].values  # Features (all columns except the target 'Revenue')\n",
    "y = df_balanced['Revenue'].values  # Target column ('Revenue')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train-test split successful!\")\n",
    "print(f\"Class distribution in training set: \\n{pd.Series(y_train).value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)  # Prevent overflow in exp\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Cost function with regularization\n",
    "def cost_function(X, y, theta, lambd=0):\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    h = np.clip(h, 1e-10, 1 - 1e-10)  # Clip to avoid log(0) or log(1)\n",
    "    regularization = (lambd / (2 * m)) * np.sum(np.square(theta[1:]))  # Regularization term\n",
    "    cost = (-1/m) * (y.T @ np.log(h) + (1 - y).T @ np.log(1 - h)) + regularization\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent with regularization\n",
    "def gradient_descent(X, y, theta, learning_rate=0.001, lambd=0, iterations=1000, tolerance=1e-4, verbose=False):\n",
    "    m = len(y)\n",
    "    costs = []\n",
    "    for i in range(iterations):\n",
    "        h = sigmoid(np.dot(X, theta))\n",
    "        gradient = (1/m) * np.dot(X.T, (h - y)) + (lambd/m) * np.concatenate([[0], theta[1:]])\n",
    "        theta -= learning_rate * gradient\n",
    "        \n",
    "        cost = cost_function(X, y, theta, lambd)\n",
    "        if i > 0 and abs(costs[-1] - cost) < tolerance:  # Early stopping based on tolerance\n",
    "            print(f\"Early stopping at iteration {i}\")\n",
    "            break\n",
    "        costs.append(cost)\n",
    "    return theta, costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function using trained theta\n",
    "def predict(X, theta, threshold=0.5):\n",
    "    return sigmoid(np.dot(X, theta)) >= threshold\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(X_train, X_test, y_train, y_test, learning_rate=0.01, iterations=5000, lambd=100, threshold=0.5):\n",
    "    # Initialize parameters\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    \n",
    "    # Train the model using gradient descent\n",
    "    trained_theta, costs = gradient_descent(X_train, y_train, theta, learning_rate, lambd, iterations, tolerance=1e-6, verbose=False)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = predict(X_test, trained_theta, threshold)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=1)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax Scaling\n",
    "def min_max_scaling(X):\n",
    "    min_vals = X.min(axis=0)\n",
    "    max_vals = X.max(axis=0)\n",
    "    ranges = max_vals - min_vals\n",
    "    ranges[ranges == 0] = 1  # Prevent division by zero\n",
    "    return (X - min_vals) / ranges\n",
    "\n",
    "# Mean Normalization\n",
    "def mean_normalization(X):\n",
    "    mean_vals = X.mean(axis=0)\n",
    "    max_vals = X.max(axis=0)\n",
    "    min_vals = X.min(axis=0)\n",
    "    denominator = (max_vals - min_vals)\n",
    "    denominator[denominator == 0] = 1  # Prevent division by zero\n",
    "    return (X - mean_vals) / denominator\n",
    "\n",
    "# Z-Score Normalization\n",
    "def z_score_normalization(X):\n",
    "    mean_vals = X.mean(axis=0)\n",
    "    std_vals = X.std(axis=0)\n",
    "    std_vals[std_vals == 0] = 1  # Prevent division by zero\n",
    "    return (X - mean_vals) / std_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_methods = {\n",
    "    'No Scaling': lambda X: X,  # No scaling is applied here, raw data is used\n",
    "    'MinMax Scaling': min_max_scaling,\n",
    "    'Mean Normalization': mean_normalization,\n",
    "    'Z-Score Normalization': z_score_normalization\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for method, scaling_function in scaling_methods.items():\n",
    "    # Scale the training data\n",
    "    X_train_scaled = scaling_function(X_train)\n",
    "    \n",
    "    # Scale the test data using the same scaling function\n",
    "    X_test_scaled = scaling_function(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy, precision, recall, f1, costs = evaluate_model(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "    \n",
    "    # Store results for comparison\n",
    "    results[method] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Costs': costs\n",
    "    }\n",
    "    \n",
    "    # Output results\n",
    "    print(f'{method} -> Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best scaling method based on results (example: MinMax Scaling)\n",
    "best_scaling_method = 'MinMax Scaling'\n",
    "X_train_scaled = scaling_methods[best_scaling_method](X_train)\n",
    "X_test_scaled = scaling_methods[best_scaling_method](X_test)\n",
    "\n",
    "# Apply regularization (e.g., lambda = 10)\n",
    "lambd = 10\n",
    "accuracy_reg, precision_reg, recall_reg, f1_reg, costs_reg = evaluate_model(X_train_scaled, X_test_scaled, y_train, y_test, lambd=lambd)\n",
    "\n",
    "# No regularization\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization\n",
    "lambd = 100 # Adjust the lambda value if needed\n",
    "accuracy_reg, precision_reg, recall_reg, f1_reg, costs_reg = evaluate_model(X_train_scaled, X_test_scaled, y_train, y_test, lambd=lambd)\n",
    "\n",
    "# No regularization\n",
    "accuracy_no_reg, precision_no_reg, recall_no_reg, f1_no_reg, costs_no_reg = evaluate_model(X_train_scaled, X_test_scaled, y_train, y_test, lambd=0)\n",
    "\n",
    "# Display final results for both regularized and non-regularized models\n",
    "print(\"\\n===== Final Results =====\")\n",
    "print(f\"Without Regularization -> Accuracy: {accuracy_no_reg:.4f}, Precision: {precision_no_reg:.4f}, Recall: {recall_no_reg:.4f}, F1 Score: {f1_no_reg:.4f}\")\n",
    "print(f\"With Regularization (lambda={lambd}) -> Accuracy: {accuracy_reg:.4f}, Precision: {precision_reg:.4f}, Recall: {recall_reg:.4f}, F1 Score: {f1_reg:.4f}\")\n",
    "\n",
    "# Plot cost function for both regularized and non-regularized models\n",
    "plt.plot(range(len(costs_no_reg)), costs_no_reg, label='Without Regularization', color='blue')\n",
    "plt.plot(range(len(costs_reg)), costs_reg, label='With Regularization', color='orange')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function over Iterations: Regularized vs Non-Regularized')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
